{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Code Final Portfolio - Component 2\n",
    "\n",
    "This notebook gives a more detailed exposition of the analyses performed in Courellis et al. (2024)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import helper_functions as F\n",
    "\n",
    "# Paths to files\n",
    "FOLDER_PATH = \"2024courellis/\"\n",
    "neu_mat_path = os.path.join(FOLDER_PATH, \"data/neu.mat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect dataset\n",
    "\n",
    "\n",
    "Each row of the `neu_data` dataset corresponds to a single neuron. \n",
    "\n",
    "* `cellinfo`: index of brain region that the cell is in;\n",
    "* `sessionID`: unique identifier for each recording session, during which paricipants performed a serial reversal-learning task;\n",
    "* `array`: a subtable of data for each neuron that contains information for all trials in the session.\n",
    "\n",
    "TO DO: what is a trial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>array</th>\n",
       "      <th>cellinfo</th>\n",
       "      <th>sessionID</th>\n",
       "      <th>n_trials</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'block_nr': 1, 'iscorrect': True, 'stim_id':...</td>\n",
       "      <td>2</td>\n",
       "      <td>P61CS_1</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'block_nr': 1, 'iscorrect': True, 'stim_id':...</td>\n",
       "      <td>2</td>\n",
       "      <td>P61CS_1</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'block_nr': 1, 'iscorrect': True, 'stim_id':...</td>\n",
       "      <td>2</td>\n",
       "      <td>P61CS_1</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'block_nr': 1, 'iscorrect': True, 'stim_id':...</td>\n",
       "      <td>4</td>\n",
       "      <td>P61CS_1</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'block_nr': 1, 'iscorrect': True, 'stim_id':...</td>\n",
       "      <td>4</td>\n",
       "      <td>P61CS_1</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>[{'block_nr': 1, 'iscorrect': False, 'stim_id'...</td>\n",
       "      <td>3</td>\n",
       "      <td>TWH172_2</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>[{'block_nr': 1, 'iscorrect': False, 'stim_id'...</td>\n",
       "      <td>3</td>\n",
       "      <td>TWH172_2</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>[{'block_nr': 1, 'iscorrect': False, 'stim_id'...</td>\n",
       "      <td>3</td>\n",
       "      <td>TWH172_2</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>[{'block_nr': 1, 'iscorrect': False, 'stim_id'...</td>\n",
       "      <td>3</td>\n",
       "      <td>TWH172_2</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>[{'block_nr': 1, 'iscorrect': False, 'stim_id'...</td>\n",
       "      <td>3</td>\n",
       "      <td>TWH172_2</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2694 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  array  cellinfo sessionID  \\\n",
       "0     [{'block_nr': 1, 'iscorrect': True, 'stim_id':...         2   P61CS_1   \n",
       "1     [{'block_nr': 1, 'iscorrect': True, 'stim_id':...         2   P61CS_1   \n",
       "2     [{'block_nr': 1, 'iscorrect': True, 'stim_id':...         2   P61CS_1   \n",
       "3     [{'block_nr': 1, 'iscorrect': True, 'stim_id':...         4   P61CS_1   \n",
       "4     [{'block_nr': 1, 'iscorrect': True, 'stim_id':...         4   P61CS_1   \n",
       "...                                                 ...       ...       ...   \n",
       "2689  [{'block_nr': 1, 'iscorrect': False, 'stim_id'...         3  TWH172_2   \n",
       "2690  [{'block_nr': 1, 'iscorrect': False, 'stim_id'...         3  TWH172_2   \n",
       "2691  [{'block_nr': 1, 'iscorrect': False, 'stim_id'...         3  TWH172_2   \n",
       "2692  [{'block_nr': 1, 'iscorrect': False, 'stim_id'...         3  TWH172_2   \n",
       "2693  [{'block_nr': 1, 'iscorrect': False, 'stim_id'...         3  TWH172_2   \n",
       "\n",
       "      n_trials  \n",
       "0          320  \n",
       "1          320  \n",
       "2          320  \n",
       "3          320  \n",
       "4          320  \n",
       "...        ...  \n",
       "2689       240  \n",
       "2690       240  \n",
       "2691       240  \n",
       "2692       240  \n",
       "2693       240  \n",
       "\n",
       "[2694 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neu_data = pd.read_json(\"neu.json\")\n",
    "neu_data[\"n_trials\"] = neu_data[\"array\"].apply(lambda x : len(x))\n",
    "neu_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_trials\n",
       "240    2309\n",
       "260     119\n",
       "200     105\n",
       "180      65\n",
       "320      57\n",
       "280      39\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neu_data[\"n_trials\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(neu_data[\"n_trials\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block_nr</th>\n",
       "      <th>iscorrect</th>\n",
       "      <th>stim_id</th>\n",
       "      <th>context</th>\n",
       "      <th>reward</th>\n",
       "      <th>response</th>\n",
       "      <th>trial_nr</th>\n",
       "      <th>fr_stim</th>\n",
       "      <th>fr_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     block_nr  iscorrect  stim_id  context  reward  response  trial_nr  \\\n",
       "0           1       True        2        2      25         0         1   \n",
       "1           1       True        4        2       5         0         2   \n",
       "2           1       True        1        2       5         1         3   \n",
       "3           1       True        2        2      25         0         4   \n",
       "4           1       True        4        2       5         0         5   \n",
       "..        ...        ...      ...      ...     ...       ...       ...   \n",
       "315        10       True        3        1       5         0        19   \n",
       "316        10       True        3        1       5         0        20   \n",
       "317        10       True        2        1      25         1        21   \n",
       "318        10       True        2        1      25         1        22   \n",
       "319        10       True        1        1      25         0        23   \n",
       "\n",
       "     fr_stim   fr_base  \n",
       "0          0  0.000000  \n",
       "1          0  0.000000  \n",
       "2          2  0.000000  \n",
       "3          1  0.909091  \n",
       "4          0  0.909091  \n",
       "..       ...       ...  \n",
       "315        3  0.909091  \n",
       "316        1  0.000000  \n",
       "317        1  1.818182  \n",
       "318        0  1.818182  \n",
       "319        3  0.909091  \n",
       "\n",
       "[320 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cell_array(\n",
    "    neu_data : pd.DataFrame,\n",
    "    cell_idx : int\n",
    " ) -> pd.DataFrame:\n",
    "    cell_array_dict = {}\n",
    "    for row_dict in neu_data[\"array\"][cell_idx]:\n",
    "        for key, value in row_dict.items():\n",
    "            if key not in cell_array_dict:\n",
    "                cell_array_dict[key] = [value]\n",
    "            else:\n",
    "                cell_array_dict[key].append(value)\n",
    "    cell_array_data = pd.DataFrame(cell_array_dict)\n",
    "    return cell_array_data\n",
    "\n",
    "cell_array_example = get_cell_array(neu_data=neu_data, cell_idx=0)\n",
    "cell_array_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform geometric analysis on balanced dichotomies\n",
    "\n",
    "We have the following parameters for running the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_resample = 5\n",
    "n_perm_inner = 1\n",
    "n_samples = [15, 15, 15, 15, 15, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Aggregate neurons by area\n",
    "\n",
    "Individual neuron data was combined across all participants to form a single \"pseudo-population.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of neurons across brain areas:\n",
      "HPC: 494\n",
      "vmPFC: 889\n",
      "AMY: 269\n",
      "dACC: 310\n",
      "preSMA: 463\n",
      "VTC: 269\n"
     ]
    }
   ],
   "source": [
    "area_order = ['HPC','vmPFC','AMY','dACC','preSMA','VTC']\n",
    "idx_order = [3, 1, 4, 2, 9, 12]\n",
    "cellinfo = neu_data.cellinfo.to_numpy()\n",
    "\n",
    "cell_area_groups = {}\n",
    "for i, area in enumerate(area_order):\n",
    "    idx = np.where(cellinfo == idx_order[i])[0]\n",
    "    cell_area_groups[area_order[i]] = idx\n",
    "\n",
    "print(\"Distribution of neurons across brain areas:\")\n",
    "for key, value in cell_area_groups.items():\n",
    "    print(f\"{key}: {len(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define metrics for assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced dichotomies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reward_25_context_2</th>\n",
       "      <th>reward_25_context_1</th>\n",
       "      <th>reward_5_context_2</th>\n",
       "      <th>reward_5_context_1</th>\n",
       "      <th>reward_0_context_2</th>\n",
       "      <th>reward_0_context_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     reward_25_context_2  reward_25_context_1  reward_5_context_2  \\\n",
       "0                   True                False               False   \n",
       "1                  False                False                True   \n",
       "2                  False                False                True   \n",
       "3                   True                False               False   \n",
       "4                  False                False                True   \n",
       "..                   ...                  ...                 ...   \n",
       "315                False                False               False   \n",
       "316                False                False               False   \n",
       "317                False                 True               False   \n",
       "318                False                 True               False   \n",
       "319                False                 True               False   \n",
       "\n",
       "     reward_5_context_1  reward_0_context_2  reward_0_context_1  \n",
       "0                 False               False               False  \n",
       "1                 False               False               False  \n",
       "2                 False               False               False  \n",
       "3                 False               False               False  \n",
       "4                 False               False               False  \n",
       "..                  ...                 ...                 ...  \n",
       "315                True               False               False  \n",
       "316                True               False               False  \n",
       "317               False               False               False  \n",
       "318               False               False               False  \n",
       "319               False               False               False  \n",
       "\n",
       "[320 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import helper_functions as F\n",
    "import itertools\n",
    "\n",
    "\n",
    "def make_variable_groups(\n",
    "    cell_data : pd.DataFrame,\n",
    "    var_names : list[str]\n",
    "):\n",
    "    \"\"\"\n",
    "    Create groups of all possible combinations of values for the given \n",
    "    variables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cell_data : pd.DataFrame\n",
    "        Data for a single neuron, where each row represents one trial.\n",
    "    var_names : list[str]\n",
    "        List of DataFrame columns to consider.\n",
    "\n",
    "    Returns:\n",
    "    groups : pd.DataFrame\n",
    "        DataFrame where each row represents a trial and each column represents \n",
    "        membership in a group.\n",
    "    \"\"\"\n",
    "    unique_values = [cell_data[var].unique() for var in var_names]\n",
    "    combinations = list(itertools.product(*unique_values))\n",
    "    group_names = [\n",
    "        \"_\".join([f\"{var}_{value}\" for var, value in zip(var_names, combo)])\n",
    "        for combo in combinations\n",
    "    ]\n",
    "    groups = pd.DataFrame(np.zeros(\n",
    "        (len(cell_data), len(combinations)), dtype=bool\n",
    "    ), columns=group_names)\n",
    "    for i, combo in enumerate(combinations):\n",
    "        # Set combination membership for all trials in a single column\n",
    "        condition = np.all([\n",
    "            cell_data[var] == value for var, value in zip(var_names, combo)\n",
    "        ], axis=0)\n",
    "        groups[group_names[i]] = condition\n",
    "    return groups\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reward_25_response_0_context_2</th>\n",
       "      <th>reward_25_response_0_context_1</th>\n",
       "      <th>reward_25_response_1_context_2</th>\n",
       "      <th>reward_25_response_1_context_1</th>\n",
       "      <th>reward_5_response_0_context_2</th>\n",
       "      <th>reward_5_response_0_context_1</th>\n",
       "      <th>reward_5_response_1_context_2</th>\n",
       "      <th>reward_5_response_1_context_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.604651</td>\n",
       "      <td>1.441176</td>\n",
       "      <td>1.967742</td>\n",
       "      <td>1.648649</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1.888889</td>\n",
       "      <td>1.659574</td>\n",
       "      <td>2.026316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.418605</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.677419</td>\n",
       "      <td>2.540541</td>\n",
       "      <td>2.194444</td>\n",
       "      <td>2.361111</td>\n",
       "      <td>2.468085</td>\n",
       "      <td>2.394737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.447368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.558140</td>\n",
       "      <td>1.852941</td>\n",
       "      <td>1.225806</td>\n",
       "      <td>1.216216</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.638889</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>1.236842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.953488</td>\n",
       "      <td>5.617647</td>\n",
       "      <td>6.516129</td>\n",
       "      <td>5.405405</td>\n",
       "      <td>6.222222</td>\n",
       "      <td>5.722222</td>\n",
       "      <td>5.446809</td>\n",
       "      <td>6.236842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.790698</td>\n",
       "      <td>10.794118</td>\n",
       "      <td>8.774194</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.527778</td>\n",
       "      <td>10.027778</td>\n",
       "      <td>8.531915</td>\n",
       "      <td>8.973684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.279070</td>\n",
       "      <td>1.176471</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>1.021277</td>\n",
       "      <td>1.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.023256</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>1.032258</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>1.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.023256</td>\n",
       "      <td>1.735294</td>\n",
       "      <td>1.290323</td>\n",
       "      <td>1.432432</td>\n",
       "      <td>1.027778</td>\n",
       "      <td>1.611111</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>1.815789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.790698</td>\n",
       "      <td>3.029412</td>\n",
       "      <td>2.548387</td>\n",
       "      <td>2.756757</td>\n",
       "      <td>3.416667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.127660</td>\n",
       "      <td>2.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.069767</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>1.277778</td>\n",
       "      <td>1.191489</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.394737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.093023</td>\n",
       "      <td>1.235294</td>\n",
       "      <td>1.161290</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>1.138889</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>1.085106</td>\n",
       "      <td>1.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.046512</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.096774</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>1.063830</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.930233</td>\n",
       "      <td>1.088235</td>\n",
       "      <td>1.193548</td>\n",
       "      <td>1.513514</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>1.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.976744</td>\n",
       "      <td>1.294118</td>\n",
       "      <td>1.258065</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>1.212766</td>\n",
       "      <td>1.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.372093</td>\n",
       "      <td>3.264706</td>\n",
       "      <td>3.612903</td>\n",
       "      <td>3.081081</td>\n",
       "      <td>3.305556</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>3.106383</td>\n",
       "      <td>2.657895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.372093</td>\n",
       "      <td>1.117647</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.763158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.921053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.790698</td>\n",
       "      <td>2.911765</td>\n",
       "      <td>2.806452</td>\n",
       "      <td>2.837838</td>\n",
       "      <td>2.916667</td>\n",
       "      <td>2.305556</td>\n",
       "      <td>2.617021</td>\n",
       "      <td>2.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.558140</td>\n",
       "      <td>1.205882</td>\n",
       "      <td>1.258065</td>\n",
       "      <td>1.432432</td>\n",
       "      <td>1.361111</td>\n",
       "      <td>1.305556</td>\n",
       "      <td>1.255319</td>\n",
       "      <td>1.710526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.534884</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.935484</td>\n",
       "      <td>1.945946</td>\n",
       "      <td>1.861111</td>\n",
       "      <td>1.972222</td>\n",
       "      <td>1.914894</td>\n",
       "      <td>1.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.837209</td>\n",
       "      <td>1.823529</td>\n",
       "      <td>1.967742</td>\n",
       "      <td>1.594595</td>\n",
       "      <td>1.805556</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>1.829787</td>\n",
       "      <td>1.815789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.744186</td>\n",
       "      <td>2.823529</td>\n",
       "      <td>2.709677</td>\n",
       "      <td>2.513514</td>\n",
       "      <td>2.444444</td>\n",
       "      <td>2.583333</td>\n",
       "      <td>2.638298</td>\n",
       "      <td>3.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.131579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.348837</td>\n",
       "      <td>2.176471</td>\n",
       "      <td>1.967742</td>\n",
       "      <td>2.378378</td>\n",
       "      <td>2.305556</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>2.595745</td>\n",
       "      <td>2.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.953488</td>\n",
       "      <td>2.352941</td>\n",
       "      <td>2.354839</td>\n",
       "      <td>2.243243</td>\n",
       "      <td>2.694444</td>\n",
       "      <td>2.055556</td>\n",
       "      <td>2.170213</td>\n",
       "      <td>2.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.488372</td>\n",
       "      <td>1.235294</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>1.216216</td>\n",
       "      <td>1.694444</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>1.276596</td>\n",
       "      <td>1.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2.395349</td>\n",
       "      <td>2.852941</td>\n",
       "      <td>3.612903</td>\n",
       "      <td>2.027027</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2.659574</td>\n",
       "      <td>2.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>1.354839</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.472222</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>1.131579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.236842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.511628</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>1.324324</td>\n",
       "      <td>1.305556</td>\n",
       "      <td>1.055556</td>\n",
       "      <td>1.127660</td>\n",
       "      <td>1.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.605263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>1.108108</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.394737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.953488</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>1.032258</td>\n",
       "      <td>1.621622</td>\n",
       "      <td>1.472222</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.106383</td>\n",
       "      <td>1.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.605263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.651163</td>\n",
       "      <td>1.058824</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>1.194444</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.319149</td>\n",
       "      <td>1.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2.348837</td>\n",
       "      <td>2.352941</td>\n",
       "      <td>1.612903</td>\n",
       "      <td>1.513514</td>\n",
       "      <td>7.694444</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.378378</td>\n",
       "      <td>1.694444</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>1.394737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2.418605</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.290323</td>\n",
       "      <td>3.081081</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.361111</td>\n",
       "      <td>1.829787</td>\n",
       "      <td>1.657895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.394737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2.744186</td>\n",
       "      <td>2.147059</td>\n",
       "      <td>2.612903</td>\n",
       "      <td>1.945946</td>\n",
       "      <td>2.694444</td>\n",
       "      <td>1.638889</td>\n",
       "      <td>2.382979</td>\n",
       "      <td>2.342105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.418605</td>\n",
       "      <td>1.235294</td>\n",
       "      <td>1.677419</td>\n",
       "      <td>1.108108</td>\n",
       "      <td>1.194444</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>1.425532</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.348837</td>\n",
       "      <td>1.323529</td>\n",
       "      <td>1.612903</td>\n",
       "      <td>1.405405</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.382979</td>\n",
       "      <td>1.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.325581</td>\n",
       "      <td>1.176471</td>\n",
       "      <td>1.322581</td>\n",
       "      <td>1.135135</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.148936</td>\n",
       "      <td>1.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>1.063830</td>\n",
       "      <td>0.763158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    reward_25_response_0_context_2  reward_25_response_0_context_1  \\\n",
       "0                         1.604651                        1.441176   \n",
       "1                         2.418605                        2.500000   \n",
       "2                         0.279070                        0.558824   \n",
       "3                         0.558140                        1.852941   \n",
       "4                         5.953488                        5.617647   \n",
       "5                         0.395349                        0.294118   \n",
       "6                         8.790698                       10.794118   \n",
       "7                         1.279070                        1.176471   \n",
       "8                         1.023256                        0.970588   \n",
       "9                         0.744186                        0.735294   \n",
       "10                        1.023256                        1.735294   \n",
       "11                        2.790698                        3.029412   \n",
       "12                        0.581395                        0.382353   \n",
       "13                        1.069767                        0.705882   \n",
       "14                        0.511628                        0.705882   \n",
       "15                        0.232558                        0.205882   \n",
       "16                        0.511628                        0.205882   \n",
       "17                        1.093023                        1.235294   \n",
       "18                        1.046512                        1.000000   \n",
       "19                        0.930233                        1.088235   \n",
       "20                        0.976744                        1.294118   \n",
       "21                        2.372093                        3.264706   \n",
       "22                        0.372093                        1.117647   \n",
       "23                        0.744186                        0.882353   \n",
       "24                        0.418605                        0.794118   \n",
       "25                        2.790698                        2.911765   \n",
       "26                        1.558140                        1.205882   \n",
       "27                        1.534884                        1.500000   \n",
       "28                        0.116279                        0.058824   \n",
       "29                        1.837209                        1.823529   \n",
       "30                        2.744186                        2.823529   \n",
       "31                        0.279070                        0.411765   \n",
       "32                        2.348837                        2.176471   \n",
       "33                        1.953488                        2.352941   \n",
       "34                        1.488372                        1.235294   \n",
       "35                        0.116279                        0.235294   \n",
       "36                        2.395349                        2.852941   \n",
       "37                        0.906977                        0.764706   \n",
       "38                        0.116279                        0.029412   \n",
       "39                        1.511628                        0.970588   \n",
       "40                        0.348837                        0.441176   \n",
       "41                        0.744186                        0.441176   \n",
       "42                        0.767442                        0.529412   \n",
       "43                        1.953488                        0.882353   \n",
       "44                        0.976744                        0.000000   \n",
       "45                        0.790698                        0.764706   \n",
       "46                        0.651163                        1.058824   \n",
       "47                        2.348837                        2.352941   \n",
       "48                        0.441860                        0.352941   \n",
       "49                        2.418605                        2.500000   \n",
       "50                        0.302326                        0.205882   \n",
       "51                        2.744186                        2.147059   \n",
       "52                        1.418605                        1.235294   \n",
       "53                        1.348837                        1.323529   \n",
       "54                        0.441860                        0.411765   \n",
       "55                        1.325581                        1.176471   \n",
       "56                        0.860465                        0.794118   \n",
       "\n",
       "    reward_25_response_1_context_2  reward_25_response_1_context_1  \\\n",
       "0                         1.967742                        1.648649   \n",
       "1                         2.677419                        2.540541   \n",
       "2                         0.580645                        0.270270   \n",
       "3                         1.225806                        1.216216   \n",
       "4                         6.516129                        5.405405   \n",
       "5                         0.354839                        0.270270   \n",
       "6                         8.774194                        9.000000   \n",
       "7                         0.935484                        0.864865   \n",
       "8                         1.032258                        0.756757   \n",
       "9                         0.741935                        0.513514   \n",
       "10                        1.290323                        1.432432   \n",
       "11                        2.548387                        2.756757   \n",
       "12                        0.483871                        0.216216   \n",
       "13                        0.967742                        0.891892   \n",
       "14                        0.612903                        0.675676   \n",
       "15                        0.032258                        0.162162   \n",
       "16                        0.419355                        0.270270   \n",
       "17                        1.161290                        1.081081   \n",
       "18                        1.096774                        0.810811   \n",
       "19                        1.193548                        1.513514   \n",
       "20                        1.258065                        0.945946   \n",
       "21                        3.612903                        3.081081   \n",
       "22                        0.645161                        0.756757   \n",
       "23                        0.612903                        0.675676   \n",
       "24                        0.419355                        0.513514   \n",
       "25                        2.806452                        2.837838   \n",
       "26                        1.258065                        1.432432   \n",
       "27                        1.935484                        1.945946   \n",
       "28                        0.129032                        0.162162   \n",
       "29                        1.967742                        1.594595   \n",
       "30                        2.709677                        2.513514   \n",
       "31                        0.451613                        0.351351   \n",
       "32                        1.967742                        2.378378   \n",
       "33                        2.354839                        2.243243   \n",
       "34                        0.935484                        1.216216   \n",
       "35                        0.322581                        0.081081   \n",
       "36                        3.612903                        2.027027   \n",
       "37                        1.354839                        0.864865   \n",
       "38                        0.129032                        0.000000   \n",
       "39                        0.741935                        1.324324   \n",
       "40                        0.451613                        0.540541   \n",
       "41                        0.451613                        1.108108   \n",
       "42                        0.354839                        0.621622   \n",
       "43                        1.032258                        1.621622   \n",
       "44                        0.161290                        0.972973   \n",
       "45                        0.645161                        0.783784   \n",
       "46                        0.483871                        0.783784   \n",
       "47                        1.612903                        1.513514   \n",
       "48                        0.225806                        0.378378   \n",
       "49                        1.290323                        3.081081   \n",
       "50                        0.322581                        0.297297   \n",
       "51                        2.612903                        1.945946   \n",
       "52                        1.677419                        1.108108   \n",
       "53                        1.612903                        1.405405   \n",
       "54                        0.258065                        0.135135   \n",
       "55                        1.322581                        1.135135   \n",
       "56                        0.741935                        0.675676   \n",
       "\n",
       "    reward_5_response_0_context_2  reward_5_response_0_context_1  \\\n",
       "0                        1.777778                       1.888889   \n",
       "1                        2.194444                       2.361111   \n",
       "2                        0.416667                       0.361111   \n",
       "3                        0.750000                       1.638889   \n",
       "4                        6.222222                       5.722222   \n",
       "5                        0.222222                       0.416667   \n",
       "6                        9.527778                      10.027778   \n",
       "7                        0.694444                       0.861111   \n",
       "8                        0.833333                       0.916667   \n",
       "9                        0.361111                       0.555556   \n",
       "10                       1.027778                       1.611111   \n",
       "11                       3.416667                       3.000000   \n",
       "12                       0.500000                       0.361111   \n",
       "13                       0.944444                       1.277778   \n",
       "14                       0.416667                       0.583333   \n",
       "15                       0.166667                       0.250000   \n",
       "16                       0.222222                       0.222222   \n",
       "17                       1.138889                       1.083333   \n",
       "18                       0.833333                       0.944444   \n",
       "19                       1.083333                       0.833333   \n",
       "20                       1.583333                       0.972222   \n",
       "21                       3.305556                       3.083333   \n",
       "22                       0.555556                       0.916667   \n",
       "23                       0.750000                       0.888889   \n",
       "24                       0.944444                       0.527778   \n",
       "25                       2.916667                       2.305556   \n",
       "26                       1.361111                       1.305556   \n",
       "27                       1.861111                       1.972222   \n",
       "28                       0.250000                       0.000000   \n",
       "29                       1.805556                       1.833333   \n",
       "30                       2.444444                       2.583333   \n",
       "31                       0.416667                       0.861111   \n",
       "32                       2.305556                       1.666667   \n",
       "33                       2.694444                       2.055556   \n",
       "34                       1.694444                       1.083333   \n",
       "35                       0.277778                       0.138889   \n",
       "36                       3.000000                       3.333333   \n",
       "37                       0.777778                       1.472222   \n",
       "38                       0.361111                       0.166667   \n",
       "39                       1.305556                       1.055556   \n",
       "40                       0.222222                       0.305556   \n",
       "41                       0.333333                       0.277778   \n",
       "42                       0.833333                       0.472222   \n",
       "43                       1.472222                       1.250000   \n",
       "44                       0.111111                       0.138889   \n",
       "45                       0.861111                       0.805556   \n",
       "46                       1.194444                       0.416667   \n",
       "47                       7.694444                       0.805556   \n",
       "48                       1.694444                       0.277778   \n",
       "49                       1.333333                       1.361111   \n",
       "50                       0.305556                       0.444444   \n",
       "51                       2.694444                       1.638889   \n",
       "52                       1.194444                       1.166667   \n",
       "53                       1.500000                       1.500000   \n",
       "54                       0.138889                       0.444444   \n",
       "55                       0.777778                       1.500000   \n",
       "56                       0.833333                       0.611111   \n",
       "\n",
       "    reward_5_response_1_context_2  reward_5_response_1_context_1  \n",
       "0                        1.659574                       2.026316  \n",
       "1                        2.468085                       2.394737  \n",
       "2                        0.468085                       0.447368  \n",
       "3                        0.468085                       1.236842  \n",
       "4                        5.446809                       6.236842  \n",
       "5                        0.191489                       0.289474  \n",
       "6                        8.531915                       8.973684  \n",
       "7                        1.021277                       1.289474  \n",
       "8                        0.872340                       1.157895  \n",
       "9                        0.744681                       0.789474  \n",
       "10                       0.978723                       1.815789  \n",
       "11                       3.127660                       2.789474  \n",
       "12                       0.531915                       0.684211  \n",
       "13                       1.191489                       0.947368  \n",
       "14                       0.851064                       0.500000  \n",
       "15                       0.191489                       0.289474  \n",
       "16                       0.319149                       0.394737  \n",
       "17                       1.085106                       1.210526  \n",
       "18                       1.063830                       0.842105  \n",
       "19                       0.957447                       1.368421  \n",
       "20                       1.212766                       1.578947  \n",
       "21                       3.106383                       2.657895  \n",
       "22                       0.744681                       0.763158  \n",
       "23                       0.744681                       0.631579  \n",
       "24                       0.617021                       0.921053  \n",
       "25                       2.617021                       2.842105  \n",
       "26                       1.255319                       1.710526  \n",
       "27                       1.914894                       1.947368  \n",
       "28                       0.127660                       0.052632  \n",
       "29                       1.829787                       1.815789  \n",
       "30                       2.638298                       3.105263  \n",
       "31                       0.234043                       0.131579  \n",
       "32                       2.595745                       2.210526  \n",
       "33                       2.170213                       2.157895  \n",
       "34                       1.276596                       1.526316  \n",
       "35                       0.148936                       0.263158  \n",
       "36                       2.659574                       2.289474  \n",
       "37                       0.723404                       1.131579  \n",
       "38                       0.212766                       0.236842  \n",
       "39                       1.127660                       1.684211  \n",
       "40                       0.340426                       0.605263  \n",
       "41                       0.297872                       0.500000  \n",
       "42                       0.446809                       0.394737  \n",
       "43                       1.106383                       1.473684  \n",
       "44                       0.042553                       0.052632  \n",
       "45                       0.702128                       0.605263  \n",
       "46                       1.319149                       1.421053  \n",
       "47                       3.000000                       7.315789  \n",
       "48                       0.361702                       1.394737  \n",
       "49                       1.829787                       1.657895  \n",
       "50                       0.319149                       0.394737  \n",
       "51                       2.382979                       2.342105  \n",
       "52                       1.425532                       0.947368  \n",
       "53                       1.382979                       1.421053  \n",
       "54                       0.255319                       0.368421  \n",
       "55                       1.148936                       1.473684  \n",
       "56                       1.063830                       0.763158  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_regressors(\n",
    "    neu_data : pd.DataFrame,\n",
    "    thr : int,\n",
    "    select : list\n",
    "):\n",
    "    \"\"\"\n",
    "    Method for balancing neuron counts between inference absent (ia) and \n",
    "    inference present (ip) groups.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    neu_data : pd.DataFrame\n",
    "        DataFrame with each row correpsonding to all trial data for a single \n",
    "        neuron.\n",
    "    thr : int\n",
    "        Minimum number of correct trials of each type to retain neurons.\n",
    "    select : list\n",
    "        Neuron indices to include.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    group_avgs = []\n",
    "    for i in select:\n",
    "        # Select only correct trials for given neuron\n",
    "        cell_data = get_cell_array(neu_data=neu_data, cell_idx=i)\n",
    "        valid_data = cell_data[cell_data[\"iscorrect\"] == True]\n",
    "        \n",
    "        # Make trial-level labels according to binary task variables\n",
    "        groups = make_variable_groups(\n",
    "            cell_data=valid_data,\n",
    "            var_names=[\"reward\", \"response\", \"context\"]\n",
    "        )     \n",
    "\n",
    "        firing_rate = valid_data[\"fr_stim\"].values\n",
    "        group_avg = []\n",
    "        for combo in groups.columns:\n",
    "            # Get boolean mask for inclusion in current group\n",
    "            group_firing_rate = firing_rate[groups[combo]]\n",
    "            # Check that neuron has enough samples of the current type\n",
    "            if len(group_firing_rate) > thr:\n",
    "                group_avg.append(group_firing_rate.mean())\n",
    "        # Only append data for neurons with enough trials in all types\n",
    "        if len(group_avg) == len(groups.columns):\n",
    "            group_avgs.append(group_avg)\n",
    "    return pd.DataFrame(np.array(group_avgs), columns=groups.columns)\n",
    "\n",
    "construct_regressors(neu_data, thr=30, select=[i for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shattering dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shattering_dimensionality(\n",
    "    avg_array,\n",
    "    n_iter : int,\n",
    "    sample_thr : int\n",
    "):\n",
    "    \"\"\"\n",
    "    Method for performing shattering dimensionality analysis \n",
    "    for a group of cells.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    avg_array : np.array\n",
    "        Cell array of neurons that exceed the trial count threshold.\n",
    "    n_iter : int\n",
    "        Number of iterations of bootstrap re-sampling to perform.\n",
    "    sample_thr : int\n",
    "        Number of trials of each condition to sample.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def sd(avg_array, nr_iterations, nr_samples):\n",
    "    \"\"\"\n",
    "    Perform shattering dimensionality analysis for a group of cells.\n",
    "\n",
    "    Parameters:\n",
    "    avg_array (list): List of neurons, with firing rates grouped hierarchically by trial type.\n",
    "    nr_iterations (int): Number of bootstrap iterations to perform.\n",
    "    nr_samples (int): Number of trials of each condition to sample.\n",
    "\n",
    "    Returns:\n",
    "    tuple: perf, boot, nr_cells\n",
    "    perf (ndarray): Decoding performance for every dichotomy across iterations.\n",
    "    boot (ndarray): Null distribution constructed using trial-label shuffle.\n",
    "    nr_cells (int): Number of cells used in analysis.\n",
    "    \"\"\"\n",
    "    PosSet, NegSet = define_sets()\n",
    "    nr_cells = len(avg_array)\n",
    "    nr_folds = 5\n",
    "\n",
    "    perf = np.full((PosSet.shape[0], nr_iterations), np.nan)\n",
    "    boot = np.full((PosSet.shape[0], nr_iterations), np.nan)\n",
    "\n",
    "    for i in range(PosSet.shape[0]):\n",
    "        gr_1 = PosSet[i]\n",
    "        gr_2 = NegSet[i]\n",
    "\n",
    "        for j in range(nr_iterations):\n",
    "            training, testing = sample_from_data(avg_array, 0, nr_samples)\n",
    "            training_data, _, labels_train, _ = prep_regressors(training, testing, gr_1, gr_2)\n",
    "\n",
    "            # Normalize and remove NaN features\n",
    "            scaler = StandardScaler()\n",
    "            training_data = scaler.fit_transform(training_data)\n",
    "            training_data = training_data[:, ~np.isnan(training_data).any(axis=0)]\n",
    "\n",
    "            # Fit logistic regression model (equivalent to linear SVM)\n",
    "            clf = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "            y_hat = cross_val_predict(clf, training_data, labels_train, cv=nr_folds)\n",
    "            perf[i, j] = accuracy_score(labels_train, y_hat)\n",
    "\n",
    "            # Compute null distribution (shuffle labels)\n",
    "            shuffled_labels = shuffle(labels_train)\n",
    "            y_hat_null = cross_val_predict(clf, training_data, shuffled_labels, cv=nr_folds)\n",
    "            boot[i, j] = accuracy_score(shuffled_labels, y_hat_null)\n",
    "\n",
    "        print(f\"Finished dichotomy {i + 1}\")\n",
    "\n",
    "    return perf, boot, nr_cells\n",
    "\n",
    "\n",
    "\n",
    "def sample_from_data(array, test_samples, train_samples):\n",
    "    \"\"\"\n",
    "    Sample data for training and testing.\n",
    "\n",
    "    Returns:\n",
    "    tuple: training, testing (both lists)\n",
    "    \"\"\"\n",
    "    testing = [None] * len(array)\n",
    "    training = [None] * len(array)\n",
    "\n",
    "    for i in range(len(array)):\n",
    "        testing[i] = []\n",
    "        training[i] = []\n",
    "\n",
    "        for trials in array[i]:\n",
    "            test_indices = np.random.choice(len(trials), test_samples, replace=False)\n",
    "            testing[i].append(trials[test_indices])\n",
    "\n",
    "            remaining_trials = np.delete(trials, test_indices)\n",
    "            train_indices = np.random.choice(len(remaining_trials), train_samples, replace=False)\n",
    "            training[i].append(remaining_trials[train_indices])\n",
    "\n",
    "    return training, testing\n",
    "\n",
    "def prep_regressors(training, testing, gr_1, gr_2):\n",
    "    \"\"\"\n",
    "    Prepare regressors for training and testing.\n",
    "\n",
    "    Returns:\n",
    "    tuple: training, testing, labels_train, labels_test\n",
    "    \"\"\"\n",
    "    training_1 = np.vstack([np.vstack(training[i][gr_1]) for i in range(len(training))])\n",
    "    training_2 = np.vstack([np.vstack(training[i][gr_2]) for i in range(len(training))])\n",
    "\n",
    "    labels_train = np.concatenate([np.ones(len(training_1)), -1 * np.ones(len(training_2))])\n",
    "\n",
    "    testing_1 = np.vstack([np.vstack(testing[i][gr_1]) for i in range(len(testing))])\n",
    "    testing_2 = np.vstack([np.vstack(testing[i][gr_2]) for i in range(len(testing))])\n",
    "\n",
    "    labels_test = np.concatenate([np.ones(len(testing_1)), -1 * np.ones(len(testing_2))])\n",
    "\n",
    "    training = np.vstack([training_1, training_2])\n",
    "    testing = np.vstack([testing_1, testing_2])\n",
    "\n",
    "    return training, testing, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run geometric analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming neu_data is a pandas DataFrame containing 'array' and 'sessions'\n",
    "# and relevant functions like construct_regressors, sd, ccgp, ps are implemented.\n",
    "\n",
    "# Initialize containers\n",
    "sd_ = [[], []]  # To store results for two conditions (inference absent, present)\n",
    "sd_boot = [[], []]\n",
    "ccgp_ = [[], []]\n",
    "ccgp_boot = [[], []]\n",
    "ps_ = [[], []]\n",
    "ps_boot = [[], []]\n",
    "\n",
    "# Example of cell_groups (brain areas)\n",
    "cell_groups = [idx_hpc, idx_pfc, idx_amy, idx_acc, idx_sma, idx_vtc]\n",
    "\n",
    "# Loop over each brain area\n",
    "for i, area_indices in enumerate(cell_groups):\n",
    "    # For inference absent sessions\n",
    "    idx_current = np.intersect1d(area_indices, np.where(np.isin(sessions, inference_absent))[0])\n",
    "\n",
    "    for _ in range(n_resample):\n",
    "        avg_array = construct_regressors(neu, n_samples[i], idx_current)\n",
    "        \n",
    "        t_1, t_2 = sd(avg_array, n_perm_inner, n_samples[i])\n",
    "        sd_[i].append(t_1)\n",
    "        sd_boot[i].append(t_2)\n",
    "        \n",
    "        ccgp_result = ccgp(avg_array, n_perm_inner, False, n_samples[i])\n",
    "        ccgp_[i].append(ccgp_result)\n",
    "        \n",
    "        ccgp_boot_result = ccgp(avg_array, n_perm_inner, True, n_samples[i])\n",
    "        ccgp_boot[i].append(ccgp_boot_result)\n",
    "        \n",
    "        ps_result = ps(avg_array, n_perm_inner, False)\n",
    "        ps_[i].append(ps_result)\n",
    "        \n",
    "        ps_boot_result = ps(avg_array, n_perm_inner, True)\n",
    "        ps_boot[i].append(ps_boot_result)\n",
    "\n",
    "    # Repeat for inference present sessions\n",
    "    idx_current = np.intersect1d(area_indices, np.where(np.isin(sessions, inference_present))[0])\n",
    "\n",
    "    for _ in range(n_resample):\n",
    "        avg_array = construct_regressors(neu, n_samples[i], idx_current)\n",
    "        \n",
    "        t_1, t_2 = sd(avg_array, n_perm_inner, n_samples[i])\n",
    "        sd_[i].append(t_1)\n",
    "        sd_boot[i].append(t_2)\n",
    "        \n",
    "        ccgp_result = ccgp(avg_array, n_perm_inner, False, n_samples[i])\n",
    "        ccgp_[i].append(ccgp_result)\n",
    "        \n",
    "        ccgp_boot_result = ccgp(avg_array, n_perm_inner, True, n_samples[i])\n",
    "        ccgp_boot[i].append(ccgp_boot_result)\n",
    "        \n",
    "        ps_result = ps(avg_array, n_perm_inner, False)\n",
    "        ps_[i].append(ps_result)\n",
    "        \n",
    "        ps_boot_result = ps(avg_array, n_perm_inner, True)\n",
    "        ps_boot[i].append(ps_boot_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
